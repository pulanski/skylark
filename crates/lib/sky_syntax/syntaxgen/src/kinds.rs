use itertools::Itertools;
use proc_macro2::{Ident, Punct, Spacing, TokenStream};
use quote::{format_ident, quote};

use crate::{add_preamble, reformat};

use super::{
    input::KindsSrc,
    sourcegen::{to_upper_snake_case, GeneratorKind},
};

pub fn generate_kinds(kinds_src: &KindsSrc) -> String {
    let (single_byte_tokens_values, single_byte_tokens) = generate_single_byte_tokens(kinds_src);

    let (punctuation_values, punctuation) = generate_punctuation(kinds_src);

    let (full_keywords_values, full_keywords) = generate_full_keywords(kinds_src);
    let (contextual_keywords_values, contextual_keywords) = generate_contextual_keywords(kinds_src);
    let (all_keywords_idents, all_keywords) =
        generate_all_keywords_idents_and_identifiers(kinds_src);

    let literals = generate_idents_from_names(kinds_src.literals);
    let tokens = generate_idents_from_names(kinds_src.tokens);
    let nodes = generate_idents_from_names(kinds_src.nodes);

    let ast = quote! {
        #![allow(bad_style, missing_docs, unreachable_pub)]
        #[allow(clippy::manual_non_exhaustive, non_snake_case, non_camel_case_types)]
        // #[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, Debug, FromPrimitive, ToPrimitive)]
        #[derive(Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash, Debug, FromPrimitive, ToPrimitive, EnumCount)]
        #[repr(u16)]
        pub enum SyntaxKind {
            // Technical SyntaxKinds: they appear temporally during parsing,
            // but never end up in the final tree
            #[doc(hidden)]
            TOMBSTONE,
            #[doc(hidden)]
            EOF,
            #[doc = "Literals (e.g. IDENTIFIER, INT, FLOAT, STRING, BYTES)"]
            #(#literals,)*
            #[doc = "Tokens (e.g. WHITESPACE, COMMENT, NEWLINE)"]
            #(#tokens,)*
            #[doc = "Keywords (e.g. BREAK, IN, LET, LOOP, etc.)"]
            #(#all_keywords,)*
            #[doc = "Punctuation (e.g. DOT, COMMA, SEMICOLON, etc.)"]
            #(#punctuation,)*
            #[doc = "Nodes (e.g. FILE, MODULE, FUNCTION, etc.)"]
            #(#nodes,)*
            // Technical kind so that we can cast from u16 safely
            #[doc(hidden)]
            __LAST,
        }
        use self::SyntaxKind::*;
        use crate::lexer::TokenKind;
        use num_derive::{FromPrimitive, ToPrimitive};
        use strum_macros::EnumCount;

        impl SyntaxKind {
            pub fn is_keyword(self) -> bool {
                matches!(self, #(#all_keywords)|*)
            }

            pub fn is_punct(self) -> bool {
                matches!(self, #(#punctuation)|*)
            }

            pub fn is_literal(self) -> bool {
                matches!(self, #(#literals)|*)
            }

            pub fn from_keyword(ident: &str) -> Option<SyntaxKind> {
                let kw = match ident {
                    #(#full_keywords_values => #full_keywords,)*
                    _ => return None,
                };
                Some(kw)
            }

            pub fn from_contextual_keyword(ident: &str) -> Option<SyntaxKind> {
                let kw = match ident {
                    #(#contextual_keywords_values => #contextual_keywords,)*
                    _ => return None,
                };
                Some(kw)
            }

            pub fn from_char(c: char) -> Option<SyntaxKind> {
                let tok = match c {
                    #(#single_byte_tokens_values => #single_byte_tokens,)*
                    _ => return None,
                };
                Some(tok)
            }
        }

        #[doc = "Converts a `TokenKind` generated by the **Lexer** to a into a `SyntaxKind` for use via the parser and other tools."]
        impl From<TokenKind> for SyntaxKind {
            fn from(kind: TokenKind) -> SyntaxKind {
                match kind {
                    #(TokenKind::#punctuation => #punctuation,)*
                    #(TokenKind::#all_keywords => #all_keywords,)*
                    #(TokenKind::#literals => #literals,)*
                    _ => unreachable!(),
                }
            }
        }

        #[macro_export]
        macro_rules! T {
            #([#punctuation_values] => { $crate::SyntaxKind::#punctuation };)*
            #([#all_keywords_idents] => { $crate::SyntaxKind::#all_keywords };)*
            [identifier] => { $crate::SyntaxKind::IDENTIFIER };
            [eof] => { $crate::SyntaxKind::EOF };
            [newline] => { $crate::SyntaxKind::NEWLINE };
            [indent] => { $crate::SyntaxKind::INDENT };
            [outdent] => { $crate::SyntaxKind::OUTDENT };
            [dslash] => { $crate::SyntaxKind::DSLASH };
        }
        pub use T;
    };

    add_preamble(reformat(ast.to_string()), GeneratorKind::Kind)
}

pub fn generate_single_byte_tokens(kinds_src: &KindsSrc) -> (Vec<char>, Vec<Ident>) {
    kinds_src
        .punct
        .iter()
        .filter(|(token, _name)| token.len() == 1)
        .map(|(token, name)| {
            (
                token
                    .chars()
                    .next()
                    .expect("Token should be a single character"),
                format_ident!("{}", name),
            )
        })
        .unzip()
}

pub fn generate_punctuation<'a>(kinds_src: &'a KindsSrc<'a>) -> (Vec<TokenStream>, Vec<Ident>) {
    let punctuation_values = kinds_src
        .punct
        .iter()
        .map(|(token, _name)| {
            if "{}[]()".contains(token) {
                let delimiter = token
                    .chars()
                    .next()
                    .expect("Token should be a single character");

                tracing::debug!("Generating delimiter: {}", delimiter);

                quote! { #delimiter }
            } else if token.eq(&"//") {
                quote! { dslash }
            } else {
                let cs = token.chars().map(|c| Punct::new(c, Spacing::Joint));
                quote! { #(#cs)* }
            }
        })
        .collect::<Vec<_>>();

    let punctuation_idents = kinds_src
        .punct
        .iter()
        .map(|(_token, name)| format_ident!("{}", name))
        .collect::<Vec<_>>();

    tracing::debug!(
        "Generated punctuation: {:?} with idents {:?}",
        punctuation_values,
        punctuation_idents
    );

    (punctuation_values, punctuation_idents)
}

pub fn generate_idents_from_names(names: &[&str]) -> Vec<Ident> {
    names
        .iter()
        .map(|name| format_ident!("{}", to_upper_snake_case(name)))
        .collect::<Vec<_>>()
}

pub fn generate_full_keywords<'a>(kinds_src: &'a KindsSrc<'a>) -> (Vec<&'a str>, Vec<Ident>) {
    let format_keyword = |name: &&'a str| format_ident!("{}_KW", to_upper_snake_case(name));

    let full_keywords_values = kinds_src.keywords.to_vec();
    let full_keywords = full_keywords_values
        .iter()
        .map(format_keyword)
        .collect_vec();

    (full_keywords_values, full_keywords)
}

pub fn generate_contextual_keywords<'a>(kinds_src: &'a KindsSrc<'a>) -> (Vec<&'a str>, Vec<Ident>) {
    let format_keyword = |name: &&'a str| format_ident!("{}_KW", to_upper_snake_case(name));

    let contextual_keywords_values = kinds_src.contextual_keywords.to_vec();
    let contextual_keywords = contextual_keywords_values
        .iter()
        .map(format_keyword)
        .collect_vec();

    (contextual_keywords_values, contextual_keywords)
}

pub fn generate_all_keywords_idents_and_identifiers<'a>(
    kinds_src: &'a KindsSrc<'a>,
) -> (Vec<Ident>, Vec<Ident>) {
    let format_keyword = |name: &&'a str| format_ident!("{}_KW", to_upper_snake_case(name));
    let format_ident_string = |name: &&'a str| format_ident!("{}", name);

    // tracing::debug!("Generating keywords ")

    let all_keywords_values = kinds_src
        .keywords
        .iter()
        .chain(kinds_src.contextual_keywords.iter())
        .copied()
        .collect::<Vec<_>>();
    let all_keywords_idents = all_keywords_values
        .iter()
        .map(format_ident_string)
        .collect::<Vec<_>>();
    let all_keywords = all_keywords_values
        .iter()
        .map(format_keyword)
        .collect::<Vec<_>>();

    (all_keywords_idents, all_keywords)
}
